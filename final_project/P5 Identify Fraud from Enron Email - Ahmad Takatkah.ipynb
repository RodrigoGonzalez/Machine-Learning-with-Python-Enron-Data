{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# P5 Identify Fraud from Enron Email - Ahmad Takatkah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question One:\n",
    "- Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. \n",
    "- As part of your answer, give some background on the dataset and how it can be used to answer the project question.\n",
    "- Were there any outliers in the data when you got it, and how did you handle those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Overview:**\n",
    "\n",
    "\n",
    "The Enron scandal, publicized in October 2001, eventually led to the largest bankruptcy reorganization in American history at that time, the Enron Corporation bankruptcy. \n",
    "\n",
    "A staff of executives at Enron were able to hide billions of dollars in debt from failed deals and projects to keep the stock price up, by the use of accounting loopholes, special purpose entities, and poor financial reporting. \n",
    "\n",
    "**Project Goal:**\n",
    "\n",
    "\n",
    "Many executives were indicted for a variety of charges and some were later sentenced to prison. This project aims to identify Enron staff who may have been involved in these fraudulent actions. To do this, the project uses a public dataset of Enron employees' financials and emails to identify a person of interest (POI). \n",
    "\n",
    "A POI is an an individual who was indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "**Using Machine Learning:**\n",
    "\n",
    "\n",
    "Supervised machine learning algorithms can take a smaller dataset of already identified POIs and process a bigger dataset to find trends and classify employees based on the provided dataset, the training data. This save time and effort and speeds up the process of investigation. \n",
    "\n",
    "The risk however would be in missing out on some false negatives (POIs that the algorithms does not classify as POIs), or have some false positives (none POIs that the algorithm mistakenly classify as POIs). \n",
    "\n",
    "**Dataset Initial Exploration:**\n",
    "\n",
    "\n",
    "The dataset includes salaries, bonuses, and other financial incentives given to Enron employees, and the history of emails sent and received by Enron employees. \n",
    "\n",
    "\n",
    "I performed a quick exploratory analysis (provided in the submitted code file) to learn more about the provided dataset, and below are the main notes with the percentage of NaN values for each feature:\n",
    "\n",
    "\n",
    "- Financial Features:\n",
    "\n",
    "|#| Feature | Missing Values Percentage |\n",
    "|---|---|---|\n",
    "| 1 | salary feature | 35.0% |\n",
    "| 2 | bonus feature | 44.0% |\n",
    "| 3 | deferral_payments feature | 73.0% |\n",
    "| 4 | total_payments feature | 14.0% |\n",
    "| 5 | exercised_stock_options feature | 30.0% |\n",
    "| 6 | restricted_stock feature | 25.0% |\n",
    "| 7 | restricted_stock_deferred feature | 88.0% |\n",
    "| 8 | total_stock_value feature | 14.0% |\n",
    "| 9 | expenses feature | 35.0% |\n",
    "| 10 | loan_advances feature | 97.0% |\n",
    "| 11 | other feature | 36.0% |\n",
    "| 12 | deferred_income feature | 66.0% |\n",
    "| 13 | long_term_incentive feature | 55.0% |\n",
    "| 14 | director_fees feature | 88.0% |\n",
    "\n",
    "- Emails Features:\n",
    "\n",
    "|#| Feature | Missing Values Percentage |\n",
    "|---|---|---|\n",
    "| 1 | to_messages feature | 41.0% |\n",
    "| 2 | from_poi_to_this_person feature | 41.0% |\n",
    "| 3 | from_messages feature | 41.0% |\n",
    "| 4 | from_this_person_to_poi feature | 41.0% |\n",
    "| 5 | shared_receipt_with_poi feature | 41.0% |\n",
    "| 6 | email_address | Ignored | \n",
    "\n",
    "\n",
    "- Labels: (Classifications):\n",
    "\n",
    "|#| Feature | Missing Values Percentage |\n",
    "|---|---|---|\n",
    "| 1 | PIO | 0.0% |\n",
    "\n",
    "- General Notes:\n",
    "    - The dataset provides details on 145 employees. \n",
    "    - There was one clear **Outlier** that was removed which is the total value for each feature. to find the outliers I sorted every features in a separate dictionary (as shown in the code file)\n",
    "    - All labels are provided (there is no missing labels) \n",
    "    - The number of POIs: 18, and the number of non-POIs: 128, \n",
    "    - 44.0% of all feature values in the dataset is missing!\n",
    "    - All provided features have missing values with different percentages (detailed in the tables above). From this we can't relay on some specific features that have most of their values missing such as: \n",
    "        - loan_advances feature (97.0% missing values)\n",
    "        - restricted_stock_deferred (88.0% missing values)\n",
    "        - director_fees feature (88.0% missing values)\n",
    "        - deferral_payments (73.0% missing values)\n",
    "        - deferred_income feature (66.0% missing values)\n",
    "        - long_term_incentive feature (55.0% missing values)\n",
    "    - Missing feature values are imputed to zero by the provided `featureFormat` function by Udacity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question Two:\n",
    "- What features did you end up using in your POI identifier, and what selection process did you use to pick them? \n",
    "- Did you have to do any scaling? Why or why not? \n",
    "- As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) \n",
    "- In your feature selection step:\n",
    "    - if you used an algorithm like a decision tree, please also give the feature importances of the features that you use\n",
    "    - if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Feature Selection:**\n",
    "\n",
    "\n",
    "I started with a big set of features (all features except the `email address`), and then, I used `SelectKBest` algorithm to select the best features. \n",
    "\n",
    "**Feature Engineering:**\n",
    "\n",
    "\n",
    "I added two new financial features: \n",
    "- salary_to_avg_salary: \n",
    "    This feature aims to show how close or far this employee's salary is from the average salary in the company. This might help in identifying a trend for overpaid executives compared to the majority of other employees and it might reduce the effect of outliers in salary that I decided to keep. \n",
    "- bonus_to_avg_bonus: \n",
    "    This feature aims to show how close or far this employee's bonus is from the average bonus in the company. This might also help in identifying a trend for overpaid executives compared to the majority of other employees and it might reduce the effect of outliers in bonus that I decided to keep.\n",
    "    \n",
    "**Feature Scaling:**\n",
    "\n",
    "\n",
    "Although I experimented with algorithms that needed feature scaling such as SVM, but for the final classifier, which was `GaussianNB`, I ended up removing scaling as it reduced the accuracy, precision and recall scores of my final classifier. \n",
    "\n",
    "\n",
    "**Final Features Selected:**\n",
    "\n",
    "\n",
    "After several experiments, in my final classifier, and I ended up using only 4 of the original features and 2 of the new features I created in a :\n",
    "- Original Features used in final classifier and their Score:\n",
    "\n",
    "| Feature | Score |\n",
    "|---|---|\n",
    "| salary | 25.0975415287 |\n",
    "| bonus | 24.4676540475 |\n",
    "| exercised_stock_options | 21.0600017075 |\n",
    "| total_stock_value | 21.0600017075 |\n",
    "| salary_to_avg_salary | 18.575703268 |\n",
    "| bonus_to_avg_bonus | 18.575703268 |\n",
    "\n",
    "\n",
    "- Engineered Features used in final classifier and their Score:\n",
    "\n",
    "| Feature | Score |\n",
    "|---|---|\n",
    "| salary_to_avg_salary | 18.575703268 |\n",
    "| bonus_to_avg_bonus | 18.575703268 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question Three:\n",
    "\n",
    "- What algorithm did you end up using? \n",
    "- What other one(s) did you try? \n",
    "- How did model performance differ between algorithms?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I used a pipeline and a grid search to experiment with 3 different algorithms with SelectKBest for feature selection. I also used the provided tester.py to measure the performance of my experiments. \n",
    "\n",
    "In the below table, I listed the used algorithms and their best performance: \n",
    "\n",
    "| Algorithm | Accuracy | Precision | Recall | F1 | F2 | Total predictions | True positives | False positives | False negatives | True negatives\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|GaussianNB()|0.85080|0.42401|0.33200|0.37241|0.34706|15000|664|902|1336|12098|\n",
    "|DecisionTreeClassifier()| 0.85200|0.37585|0.16650|0.23077|0.18737|15000|333|553|1667|12447|\n",
    "|SVC()|0.87513|0.65526|0.13400|0.22250|0.15935|15000|268|141|1732|12859|\n",
    "\n",
    "\n",
    "I ended up choosing GaussianNB() with SelectKBest because it gave me the highest values for both precision and recall above 0.3 as required. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question Four:\n",
    "\n",
    "- What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  \n",
    "- How did you tune the parameters of your particular algorithm? \n",
    "- What parameters did you tune? \n",
    "     - (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Parameter Tuning:**\n",
    "\n",
    "\n",
    "ML algorithms come with specific parameters whose values can be changed (tuned) to achieve the best performance possible. Not tuning the parameters well, can result in poor performance or in getting unexpected results because the algorithm couldn't train well on the provided dataset. \n",
    "\n",
    "\n",
    "**Used Algorithms and Their Parameters:**\n",
    "\n",
    "\n",
    "I used a pipeline and a grid search to experiment with 3 different algorithms:\n",
    "- SelectKBest() (Which I ended up using along with GaussianNB())\n",
    "    - For feature selection, I experimented with the following parameter values:\n",
    "        - 'selectkbest__k': range(2,22) (based on the number of features in the features_list and excluding the first one which is the Label: POI)\n",
    "\n",
    "\n",
    "- GaussianNB() (Which I ended up using along with SelectKBest())\n",
    "    - For naive bayes there were no parameters to tune\n",
    " \n",
    "    \n",
    "- DecisionTreeClassifier()\n",
    "    - For decision trees, I experimented with the following parameter values:\n",
    "        - 'tree__criterion' : ['gini', 'entropy'],\n",
    "        - 'tree__max_depth' : [None, 1, 2, 3, 4],\n",
    "        - 'tree__min_samples_split' : [2, 3, 4, 25],\n",
    "        \n",
    "        \n",
    "- SVC()\n",
    "    - For support vector machines,I experimented with the following parameter values:\n",
    "        - 'svm__kernel' : ['rbf'],\n",
    "        - 'svm__C' : [1, 10, 100, 1000, 10000],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question Five:\n",
    "\n",
    "- What is validation, and what’s a classic mistake you can make if you do it wrong? \n",
    "- How did you validate your analysis?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Validation is basically testing the process of training a ML algorithm. Mistakes can vary, but the most common one would be using the same data set for both training and testing the algorithm. to avoid this, the dataset is usually split into two separate sets: a training set and a testing set. \n",
    "\n",
    "I used the train_test_split method to split the provided dataset (30% test, - 70% train), and then the GridSearchCV method to create multiple combinations of train_test datasets splits. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question Six:\n",
    "\n",
    "- Give at least 2 evaluation metrics and your average performance for each of them.  \n",
    "- Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Evaluation Metrics:**\n",
    "\n",
    "\n",
    "Among other evaluation metrics, I can list precision and recall as two main measures of relevance for the performance of a machine learning algorithm.\n",
    "\n",
    "\n",
    "For my algorithm (GaussianNB()), here are the values for those two measures:\n",
    "   - Precision: 0.42401\n",
    "   - Recall: 0.33200\n",
    "\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "\n",
    "precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances. Meaning, how accurate is the classifier in classifying a person of interest when the person is truly a person of interest. \n",
    "\n",
    "while recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over total relevant instances. Meaning, the rate at which the classifier can classify a person of interest among all true persons of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### References:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Sorting dictionaries: https://stackoverflow.com/questions/613183/sort-a-python-dictionary-by-value\n",
    "- Access an arbitrary element in a dictionary in Python: https://stackoverflow.com/questions/3097866/access-an-arbitrary-element-in-a-dictionary-in-python\n",
    "- Precision and recall: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "- SelectKBest Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "- GaussianNB Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html \n",
    "- DecisionTreeClassifier Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- SVC Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
