{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lenght of the dataset:  146\n",
      "\n",
      " NaN values are 44.0% of all feature values in the whole dataset!\n",
      "3066\n",
      "\n",
      " poi feature has 0.0% NaN values.\n",
      "\n",
      " salary feature has 35.0% NaN values.\n",
      "\n",
      " bonus feature has 44.0% NaN values.\n",
      "\n",
      " deferral_payments feature has 73.0% NaN values.\n",
      "\n",
      " total_payments feature has 14.0% NaN values.\n",
      "\n",
      " exercised_stock_options feature has 30.0% NaN values.\n",
      "\n",
      " restricted_stock feature has 25.0% NaN values.\n",
      "\n",
      " restricted_stock_deferred feature has 88.0% NaN values.\n",
      "\n",
      " total_stock_value feature has 14.0% NaN values.\n",
      "\n",
      " expenses feature has 35.0% NaN values.\n",
      "\n",
      " loan_advances feature has 97.0% NaN values.\n",
      "\n",
      " other feature has 36.0% NaN values.\n",
      "\n",
      " deferred_income feature has 66.0% NaN values.\n",
      "\n",
      " long_term_incentive feature has 55.0% NaN values.\n",
      "\n",
      " director_fees feature has 88.0% NaN values.\n",
      "\n",
      " to_messages feature has 41.0% NaN values.\n",
      "\n",
      " from_poi_to_this_person feature has 41.0% NaN values.\n",
      "\n",
      " from_messages feature has 41.0% NaN values.\n",
      "\n",
      " from_this_person_to_poi feature has 41.0% NaN values.\n",
      "\n",
      " shared_receipt_with_poi feature has 41.0% NaN values.\n",
      "\n",
      " One entry in the dataset: \n",
      "('METTS MARK', {'salary': 365788, 'to_messages': 807, 'deferral_payments': 'NaN', 'total_payments': 1061827, 'exercised_stock_options': 'NaN', 'bonus': 600000, 'restricted_stock': 585062, 'shared_receipt_with_poi': 702, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 585062, 'expenses': 94299, 'loan_advances': 'NaN', 'from_messages': 29, 'other': 1740, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'mark.metts@enron.com', 'from_poi_to_this_person': 38})\n",
      "\n",
      " Number of features per person:  21\n",
      "\n",
      " Number of POIs: 18, \n",
      " Number of non-POIs: 128, \n",
      " Total Provided: 146, \n",
      " Missing Classifications: 0\n",
      "\n",
      " Sorted salary feature by values (performed for every feature to find outliers): \n",
      "[('TOTAL', 26704229), ('SKILLING JEFFREY K', 1111258), ('LAY KENNETH L', 1072321), ('FREVERT MARK A', 1060932), ('PICKERING MARK R', 655037), ('WHALLEY LAWRENCE G', 510364), ('DERRICK JR. JAMES V', 492375), ('FASTOW ANDREW S', 440698), ('SHERRIFF JOHN R', 428780), ('RICE KENNETH D', 420636), ('CAUSEY RICHARD A', 415189), ('KEAN STEVEN J', 404338), ('HAEDICKE MARK E', 374125), ('MCMAHON JEFFREY', 370448), ('METTS MARK', 365788), ('DELAINEY DAVID W', 365163), ('MCCONNELL MICHAEL S', 365038), ('WALLS JR ROBERT H', 357091), ('MARTIN AMANDA K', 349487), ('LAVORATO JOHN J', 339288), ('BUY RICHARD B', 330546), ('OLSON CINDY K', 329078), ('WHITE JR THOMAS E', 317543), ('COX DAVID', 314288), ('KOENIG MARK E', 309946), ('FALLON JAMES B', 304588), ('SHANKMAN JEFFREY A', 304110), ('UMANOFF ADAM S', 288589), ('JACKSON CHARLENE R', 288558), ('COLWELL WESLEY', 288542), ('BOWEN JR RAYMOND M', 278601), ('DONAHUE JR JEFFREY M', 278601), ('KAMINSKI WINCENTY J', 275101), ('GLISAN JR BEN F', 274975), ('LEFF DANIEL P', 273746), ('GOLD JOSEPH', 272880), ('KITCHEN LOUISE', 271442), ('SHAPIRO RICHARD S', 269076), ('BAXTER JOHN C', 267102), ('MORDAUNT KRISTINA M', 267093), ('TAYLOR MITCHELL S', 265214), ('MCCLELLAN GEORGE', 263413), ('DIMICHELE RICHARD G', 262788), ('HERMANN ROBERT J', 262663), ('PAI LOU L', 261879), ('CARTER REBECCA C', 261809), ('BUTTS ROBERT H', 261516), ('WASAFF GEORGE', 259996), ('SUNDE MARTIN', 257486), ('MULLER MARK S', 251654), ('DIETRICH JANET R', 250100), ('RIEKER PAULA H', 249201), ('BLACHMAN JEREMY M', 248546), ('SHARP VICTORIA T', 248146), ('BUCHANAN HAROLD G', 248017), ('TILNEY ELIZABETH A', 247338), ('HANNON KEVIN P', 243293), ('CALGER CHRISTOPHER F', 240189), ('BAY FRANKLIN R', 239671), ('STABLER FRANK', 239502), ('LINDHOLM TOD A', 236457), ('GARLAND C KEVIN', 231946), ('BECK SALLY W', 231330), ('MURRAY JULIA H', 229284), ('KOPPER MICHAEL J', 224305), ('THORN TERENCE H', 222093), ('DODSON KEITH', 221003), ('BERBERIAN DAVID', 216582), ('BELDEN TIMOTHY N', 213999), ('BIBI PHILIPPE A', 213625), ('SHELBY REX', 211844), ('HICKERSON GARY J', 211788), ('DURAN WILLIAM D', 210692), ('DETMERING TIMOTHY J', 210500), ('DEFFNER JOSEPH M', 206121), ('ALLEN PHILLIP K', 201955), ('FITZGERALD JAY L', 199157), ('PIPER GREGORY F', 197091), ('GAHN ROBERT S', 192008), ('BERGSIEKER RICHARD P', 187922), ('CUMBERLAND MICHAEL S', 184899), ('ECHOLS JOHN B', 182245), ('KISHKILL JOSEPH G', 174246), ('ELLIOTT STEVEN', 170941), ('SULLIVAN-SHAKLOVITZ COLLEEN', 162779), ('YEAGER F SCOTT', 158403), ('HUMPHREY GENE E', 130724), ('REDMOND BRIAN L', 96840), ('OVERDYKE JR JERE C', 94941), ('IZZO LAWRENCE L', 85274), ('BAZELIDES PHILIP J', 80818), ('REYNOLDS LAWRENCE', 76399), ('WESTFAHL RICHARD K', 63744), ('GRAY RODNEY', 6615), ('BANNANTINE JAMES M', 477)]\n",
      "\n",
      " The TOTAL entry row: \n",
      "{'salary': 26704229, 'to_messages': 'NaN', 'deferral_payments': 32083396, 'total_payments': 309886585, 'exercised_stock_options': 311764000, 'bonus': 97343619, 'restricted_stock': 130322299, 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': -7576788, 'total_stock_value': 434509511, 'expenses': 5235198, 'loan_advances': 83925000, 'from_messages': 'NaN', 'other': 42667589, 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 1398517, 'deferred_income': -27992891, 'long_term_incentive': 48521928, 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}\n",
      "\n",
      " All Salaries:  26704229\n",
      "\n",
      " All Bonuses:  97343619\n",
      "\n",
      "Pipeline(steps=[('selectkbest', SelectKBest(k=6, score_func=<function f_classif at 0x1178339b0>)), ('naive_bayes', GaussianNB(priors=None))])\n",
      "\n",
      "Best parameters are:  {'selectkbest__k': 6} \n",
      "\n",
      "The  6  features selected and their scores:\n",
      "feature no. 1: salary (25.0975415287)\n",
      "feature no. 2: bonus (24.4676540475)\n",
      "feature no. 3: exercised_stock_options (21.0600017075)\n",
      "feature no. 4: total_stock_value (21.0600017075)\n",
      "feature no. 5: salary_to_avg_salary (18.575703268)\n",
      "feature no. 6: bonus_to_avg_bonus (18.575703268)\n",
      "\n",
      " Udacity Tester results: \n",
      "\n",
      "Pipeline(steps=[('selectkbest', SelectKBest(k=6, score_func=<function f_classif at 0x1178339b0>)), ('naive_bayes', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.85080\tPrecision: 0.42401\tRecall: 0.33200\tF1: 0.37241\tF2: 0.34706\n",
      "\tTotal predictions: 15000\tTrue positives:  664\tFalse positives:  902\tFalse negatives: 1336\tTrue negatives: 12098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from math import isnan\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import main\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------#\n",
    "### Task 1: Select what features you'll use.\n",
    "#---------------------------------------------------------------------#\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "features_list = [\n",
    "                 'poi',\n",
    "                 'salary',\n",
    "                 'bonus',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'exercised_stock_options',\n",
    "                 'restricted_stock',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'loan_advances',\n",
    "                 'other',\n",
    "                 'deferred_income',\n",
    "                 'long_term_incentive',\n",
    "                 'director_fees',\n",
    "                 'to_messages',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'salary_to_avg_salary', # new feature to be created\n",
    "                 'bonus_to_avg_bonus', # new feature to be created\n",
    "                ] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "\n",
    "# Print the length of the dataset:\n",
    "print '\\n Lenght of the dataset: ', len(data_dict)\n",
    "\n",
    "# Count the number of All NaN values in the whole dataset:\n",
    "all_values = 0\n",
    "all_NaNs = 0\n",
    "for name, features in data_dict.iteritems():\n",
    "    for k, v in features.iteritems():\n",
    "        all_values +=1\n",
    "        if v == 'NaN':\n",
    "            all_NaNs +=1\n",
    "\n",
    "print '\\n NaN values are {}% of all feature values in the whole dataset!'.\\\n",
    "        format(round(float(all_NaNs)/float(all_values)*100))\n",
    "print all_values\n",
    "\n",
    "# Count the number of NaN values for each feature:\n",
    "def count_NaNs(feature_name):\n",
    "    feature_values = 0\n",
    "    feature_NaNs = 0\n",
    "    for name, features in data_dict.iteritems():\n",
    "        if features[feature_name] == 'NaN':\n",
    "            feature_NaNs +=1\n",
    "        elif features[feature_name] != 'NaN':\n",
    "            feature_values +=1             \n",
    "    print '\\n {} feature has {}% NaN values.'.\\\n",
    "        format(feature_name, round(float(feature_NaNs)/float(feature_NaNs+feature_values)*100))\n",
    "\n",
    "for feature in features_list[0:20]:\n",
    "    count_NaNs(feature)\n",
    "    \n",
    "    \n",
    "# print one of the key/value pairs in the dictionary to learn more about it:\n",
    "print '\\n One entry in the dataset: \\n', data_dict.items()[0]\n",
    "\n",
    "# printing the length of features for evey person:\n",
    "print '\\n Number of features per person: ', len(data_dict.values()[0])\n",
    "\n",
    "# printing the number of missing Persons of Interest classifications provided:\n",
    "POIs = 0\n",
    "nPOIs = 0\n",
    "for k, v in data_dict.iteritems():\n",
    "    if v[\"poi\"] == 1:  # (or True)\n",
    "        POIs += 1\n",
    "    else: \n",
    "        nPOIs += 1\n",
    "# OR \n",
    "#POIs = sum(person['poi'] for person in data_dict.values())\n",
    "#nPOIs = sum(not person['poi'] for person in data_dict.values())       \n",
    "print \"\\n Number of POIs: {}, \\n Number of non-POIs: {}, \\n Total Provided: {}, \\n Missing Classifications: {}\".\\\n",
    "        format(POIs, nPOIs, POIs+nPOIs, len(data_dict)-(POIs+nPOIs))\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------#\n",
    "### Task 2: Remove outliers\n",
    "#---------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# Separate a feature in a new dictionary to find it's outliers (repeated for important features):\n",
    "temp_dict = defaultdict(int)\n",
    "temp_feature = 'salary'\n",
    "\n",
    "for name, features in data_dict.iteritems():\n",
    "    temp_dict[name] = data_dict[name][temp_feature]\n",
    "\n",
    "temp_dict = {k: temp_dict[k] for k in temp_dict if not isnan(float(temp_dict[k]))}\n",
    "\n",
    "sorted_temp_dict_by_value = sorted(temp_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print '\\n Sorted {} feature by values (performed for every feature to find outliers): '.format(temp_feature)\n",
    "print sorted_temp_dict_by_value\n",
    "\n",
    "#sorted_temp_dict_by_key = sorted(temp_dict.items(), key=operator.itemgetter(0))\n",
    "#print sorted_temp_dict_by_key\n",
    "#print '\\n'\n",
    "\n",
    "\n",
    "# Print the entry in the dataset of the name 'Total':\n",
    "print '\\n The TOTAL entry row: \\n', data_dict['TOTAL']\n",
    "\n",
    "# Removing the Total entry:\n",
    "data_dict.pop(\"TOTAL\")\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------#\n",
    "### Task 3: Create new feature(s)\n",
    "#---------------------------------------------------------------------#\n",
    "\n",
    "# Calculat the sum of all salaries:\n",
    "all_salaries = 0    \n",
    "for k, v in data_dict.iteritems():\n",
    "    if v[\"salary\"] != 'NaN':  \n",
    "        all_salaries = all_salaries + v[\"salary\"]\n",
    "    else: \n",
    "        pass\n",
    "print '\\n All Salaries: ', all_salaries\n",
    "\n",
    "# Create a new feature 'salary_to_avg_salary':\n",
    "for name, features in data_dict.iteritems():\n",
    "    try:\n",
    "        features['salary_to_avg_salary'] = float(features['salary']) / float(all_salaries)\n",
    "    except:\n",
    "        pass\n",
    "    if isnan(features['salary_to_avg_salary']):\n",
    "        features['salary_to_avg_salary']= 0\n",
    "\n",
    "        \n",
    "# Calculat the sum of all bonuses:\n",
    "all_bonuses = 0    \n",
    "for k, v in data_dict.iteritems():\n",
    "    if v[\"bonus\"] != 'NaN':  \n",
    "        all_bonuses = all_bonuses + v[\"bonus\"]\n",
    "    else: \n",
    "        pass\n",
    "print '\\n All Bonuses: ', all_bonuses\n",
    "\n",
    "# Create a new feature 'bonus_to_avg_bonus':\n",
    "for name, features in data_dict.iteritems():\n",
    "    try:\n",
    "        features['bonus_to_avg_bonus'] = float(features['bonus']) / float(all_bonuses)\n",
    "    except:\n",
    "        pass\n",
    "    if isnan(features['bonus_to_avg_bonus']):\n",
    "        features['bonus_to_avg_bonus']= 0\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "#---------------------------------------------------------------------#\n",
    "### Task 4: Try a varity of classifiers\n",
    "#---------------------------------------------------------------------#\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "# Import needed methods\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "NB = GaussianNB()\n",
    "DT = DecisionTreeClassifier()\n",
    "SV = SVC()\n",
    "\n",
    "# Set the parameters for all used algorithms to use in Task 5:\n",
    "param_grid = {\n",
    "              'selectkbest__k': range(2,22),\n",
    "              #'tree__random_state' : [42],\n",
    "              #'tree__criterion' : ['gini', 'entropy'],\n",
    "              #'tree__max_depth' : [None, 1, 2, 3, 4],\n",
    "              #'tree__min_samples_split' : [2, 3, 4, 25],\n",
    "              #'svm__kernel' : ['rbf'],\n",
    "              #'svm__C' : [1, 10, 100, 1000, 10000],\n",
    "              }\n",
    "\n",
    "# Create the pipline to use in Task 5\n",
    "pipeline = Pipeline([\n",
    "                    #('min_max_scaler', scaler),\n",
    "                    ('selectkbest', SelectKBest()),\n",
    "                    ('naive_bayes', NB),\n",
    "                    #('tree', DT),\n",
    "                    #('svm', SV),\n",
    "                    ])\n",
    "                  \n",
    "\n",
    "#---------------------------------------------------------------------#\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "#---------------------------------------------------------------------#\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "#clf = clf.fit(features_train, labels_train)     \n",
    "#pred = clf.predict(features_test)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, cv=5, n_jobs=1, param_grid=param_grid)\n",
    "grid_search.fit(features, labels)\n",
    "clf = grid_search.best_estimator_\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "print '\\n', clf\n",
    "print '\\n', \"Best parameters are: \", grid_search.best_params_, '\\n'\n",
    "\n",
    "selected_features=[features_list[i+1] for i in clf.named_steps['selectkbest'].get_support(indices=True)]\n",
    "#print '\\n Selected Features: ', selected_features\n",
    "scores = clf.named_steps['selectkbest'].scores_\n",
    "#print '\\n Scores of Selected Features: ', scores\n",
    "\n",
    "indices = np.argsort(scores)[::-1]\n",
    "#print indices\n",
    "print 'The ', len(selected_features), \" features selected and their scores:\"\n",
    "for i in range(len(selected_features)):\n",
    "    #print i\n",
    "    #print indices[i]\n",
    "    print \"feature no. {}: {} ({})\".format(i+1,selected_features[i], scores[indices[i]])\n",
    "\n",
    "\n",
    "#print \"grid search results: \\n\"\n",
    "#print grid_search.cv_results_\n",
    "\n",
    "#---------------------------------------------------------------------#\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. \n",
    "#---------------------------------------------------------------------#\n",
    "### You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "\n",
    "print '\\n Udacity Tester results: \\n'\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
